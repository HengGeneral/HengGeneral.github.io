---
layout: post
title: 决策树
tags:  [机器学习]
categories: [机器学习]
author: liheng
excerpt: "Machine Learning"
---
## 决策树

### 介绍

决策树学习是一种近似估计离散函数的方法, 经过学习的离散函数就代表了一棵决策树。
另外, 该决策树表示为 if-then 语句的集合时, 也能更易于人们理解。
目前,这些学习方法在归纳推理算法中极其流行, 同时也广泛地应用于疾病监测和信贷风险评估等领域。

### 决策树的表示方法

决策树将每个实体(instance)沿着树根到叶子节点进行判断, 这样就可以得到该实体的分类。
决策树种的每个结点表示了实体某种属性的判断, 结点的每个分支则对应了该属性的一个可能的取值。

### 适合决策树学习的问题

决策树学习方法通常适合运用于具有如下特点的问题中:

1. 实例都是以key-value形式的。
2. 目标函数的结果是离散的。比如true/false输出, 或者更多的输出结果, 甚至是functions with real-valued outputs, 尽管这种情况并不常见。
3. 训练集可能含有错误数据。决策树学习方法对错误数据具有鲁棒性,这里的错误数据包含训练集数据的分类和属性的错误值。
4. 训练集中某些属性的值可能缺失。

### 算法

大部分决策树算法都是为了构建一个自上而下的, 贪心搜索的决策树。那么问题是, 哪个属性应该放在决策树的根节点进行检验?

为了回答这个问题, 需要找到训练集中最优的属性作为根结点, 该结点的每个分支是基于该属性不同的取值, 然后对该结点的各个子结点用相应
的方法。

#### 选择哪个属性作为最优的分类器?

首先选择一种统计特性--信息增益(information gain), 去度量各属性作为分类器的效果。

##### 熵(Entropy)

为了更准确地定义信息增益(information gain), 信息论中的熵被用来表示训练集中数据的多样性程度。
给定集合S, 该集合包含两种分类结果, yes 和 no, 那么S的熵则是:

$$ Entroy(S) = - p_{y} log_{2} p_{y} - p_{n} log_{2} p_{n} $$

其中, $$p_{y}$$ 表示 S中yes的占比, $$p_{n}$$ 表示 S 中no的占比。






参考文献:

1. https://www.cs.princeton.edu/courses/archive/spring07/cos424/papers/mitchell-dectrees.pdf